1001 miles
============  

cProfile
--------------

cProfile是python的性能测试模块，它只是_lsprof模块的一个封装，用来展示输出后者收集的数据。

运行profile实际上是在enable，disable Python VM的profiling功能。

Lib/cProfile.py::

    class Profile(_lsprof.Profiler):
        """Profile(custom_timer=None, time_unit=None, subcalls=True, builtins=True)

        ...
     
        def runctx(self, cmd, globals, locals):
            self.enable()
            try:
                exec cmd in globals, locals
            finally:
                self.disable()
            return self

Module/_lsprof.c::

    static PyObject*
    profiler_enable(ProfilerObject *self, PyObject *args, PyObject *kwds)
    {
        int subcalls = -1;
        int builtins = -1;
        static char *kwlist[] = {"subcalls", "builtins", 0};
        if (!PyArg_ParseTupleAndKeywords(args, kwds, "|ii:enable",
                                         kwlist, &subcalls, &builtins))
            return NULL;
        if (setSubcalls(self, subcalls) < 0 || setBuiltins(self, builtins) < 0)
            return NULL;
        PyEval_SetProfile(profiler_callback, (PyObject*)self);
        self->flags |= POF_ENABLED;
        Py_INCREF(Py_None);
        return Py_None;
    }

调用PyEval_SetProfile设置了一个callback profiler_callback, 这样python vm在进入函数，
从函数返回前就会告诉我们::

    static int
    profiler_callback(PyObject *self, PyFrameObject *frame, int what,
                      PyObject *arg)
    {
        switch (what) {

        /* the 'frame' of a called function is about to start its execution */
        case PyTrace_CALL:
            ptrace_enter_call(self, (void *)frame->f_code,
                                   (PyObject *)frame->f_code);

            break;

        /* the 'frame' of a called function is about to finish
           (either normally or with an exception) */
        case PyTrace_RETURN:
            ptrace_leave_call(self, (void *)frame->f_code);
            break;

        /* case PyTrace_EXCEPTION:
            If the exception results in the function exiting, a
            PyTrace_RETURN event will be generated, so we don't need to
            handle it. */

    #ifdef PyTrace_C_CALL   /* not defined in Python <= 2.3 */
        /* the Python function 'frame' is issuing a call to the built-in
           function 'arg' */
        case PyTrace_C_CALL:
                ...
    #endif
        ...

最重要是PyTrace_CALL, PyTrace_RETURN这两个信号，分别表示将要进入和返回函数。
详细请参考 http://docs.python.org/release/2.6.7/c-api/init.html#PyTrace_CALL

要搞清楚ptrace_enter_call, ptrace_leave_call怎么回事，需要明白两个数据结构::

    /* represents a function or user defined block */
    typedef struct _ProfilerEntry {
        rotating_node_t header;
        PyObject *userObj; /* PyCodeObject, or a descriptive str for builtins */
        PY_LONG_LONG tt; /* total time in this entry */
        PY_LONG_LONG it; /* inline time in this entry (not in subcalls) */
        long callcount; /* how many times this was called */
        long recursivecallcount; /* how many times called recursively */
        long recursionLevel;
        rotating_node_t *calls;
    } ProfilerEntry;

    typedef struct _ProfilerContext {
        PY_LONG_LONG t0;
        PY_LONG_LONG subt;
        struct _ProfilerContext *previous;
        ProfilerEntry *ctxEntry;
    } ProfilerContext;


- ProfilerContext

  可以认为时调用堆栈链，previous指向上层调用者。存放单次计时的状态，比如进入该函数的时间t0，所有子函数耗时subt，
  这两个数据在退出函数时即Stop函数中，用来计算本次调用的tt以及it，然后累加到该函数对应的全局entry中。

- ProfilerEntry

  计时汇总信息，每个callable只对应一个entry，在这里含有所有该函数的性能数据，如
  调用次数callcount，递归调用次数recursivecallcount，当前递归深度recursionLevel，总耗时tt，去除subcall耗时之后该函数自身耗时it等


foo递归调用自己，然后又调用foo1，则上面的结构看起来如下::

    Context:                    Entry:

    foo                         foo
    foo                         foo1
    foo
    foo
    foo1    
      
开始时间，结束时间分别在initContext, Stop中获得，调用CALL_TIMER(pObj)，单位为微秒，参见 hpTimer()。

以下是在进入，退出函数时打印一些信息的patch::

    diff --git a/Modules/_lsprof.c b/Modules/_lsprof.c
    index 049c94d..53819ae 100644
    --- a/Modules/_lsprof.c
    +++ b/Modules/_lsprof.c
    @@ -319,6 +319,10 @@ static void clearEntries(ProfilerObject *pObj)
     static void
     initContext(ProfilerObject *pObj, ProfilerContext *self, ProfilerEntry *entry)
     {
    +    if (PyCode_Check(entry->userObj)){ #要进入的函数可能不是PyCodeObject类型，比如上面的PyTrace_C_CALL
    +        printf("Entering func %s\n", PyString_AS_STRING(((PyCodeObject *)entry->userObj)->co_name));
    +    }
    +
         self->ctxEntry = entry;
         self->subt = 0;
         self->previous = pObj->currentProfilerContext;
    @@ -339,17 +343,30 @@ initContext(ProfilerObject *pObj, ProfilerContext *self, ProfilerEntry *entry)
     static void
     Stop(ProfilerObject *pObj, ProfilerContext *self, ProfilerEntry *entry)
     {
    +    // Total time spent in this level of recursion of a function
         PY_LONG_LONG tt = CALL_TIMER(pObj) - self->t0;
    +    // Pure time not included sub calls
         PY_LONG_LONG it = tt - self->subt;
         if (self->previous)
             self->previous->subt += tt;   # 把本次调用的总耗时算到上一层调用者的子调用耗时里，这样上面的it=tt->self.subt就说的通了
         pObj->currentProfilerContext = self->previous;
    +
    +    // Increase the time spent  in a function after all recursion is over
         if (--entry->recursionLevel == 0)
             entry->tt += tt; # 累加 
         else
             ++entry->recursivecallcount;
    +
    +    // Increase pure time every recursion
         entry->it += it; # 累加
         entry->callcount++;
    +    double collect_factor = hpTimerUnit();
    +
    +    if (PyCode_Check(entry->userObj)){
    +        printf("Leaving func %20s  ", PyString_AS_STRING(((PyCodeObject *)entry->userObj)->co_name));
    +        printf("Timers: tt %.4f, it %.4f, nc %d, rl %d\n", entry->tt * collect_factor, 
    +            entry->it * collect_factor, entry->callcount, entry->recursionLevel);
    +    }
         if ((pObj->flags & POF_SUBCALLS) && self->previous) {
             /* find or create an entry for me in my caller's entry */
             ProfilerEntry *caller = self->previous->ctxEntry;
    @@ -441,7 +458,8 @@ profiler_callback(PyObject *self, PyFrameObject *frame, int what,
         /* the 'frame' of a called function is about to start its execution */
         case PyTrace_CALL:
             ptrace_enter_call(self, (void *)frame->f_code,
    -                                (PyObject *)frame->f_code);
    +                               (PyObject *)frame->f_code);
    +
             break;
     
         /* the 'frame' of a called function is about to finish
    @@ -593,7 +611,7 @@ static int statsForEntry(rotating_node_t *node, void *arg)
                                      entry->userObj,
                                      entry->callcount,
                                      entry->recursivecallcount,
    -                                 collect->factor * entry->tt,
    +                                 collect->factor * entry->tt, // NOTE
                                      collect->factor * entry->it,
                                      collect->sublist);
         Py_DECREF(collect->sublist);
    @@ -628,6 +646,7 @@ profiler_subentry objects:\n\
         inlinetime    inline time (not in further subcalls)\n\
     ");
     
    +
     static PyObject*
     profiler_getstats(ProfilerObject *pObj, PyObject* noarg)
     {
    20:46 jaime@oldtown Python-2.6.7 (cprofile)$ 

用来profile的测试文件， test.py::

    import time

    def foo1():
        time.sleep(1)

    def foo(n):
        foo1()
        if n > 0:
            return foo(n - 1)
        t = 1
        i = 1
        while i< 10000:
            i += 1
            t *= i
        return 42

    class A:
        def test(self):
            foo(3)

    print 'foo', id(foo)
    print 'foo1', id(foo1)

    a = A()
    print 'A.test', id(a.test)
    print 'A.test', id(A().test)
    a.test()

foo递归调用自己，每次都调用foo1。为了区别，我们在最后一次调用foo时做了一些计算，这次调用自身也消耗一些时间。profile以可执行的函数为最小单位来计算耗时，每个callable都是一个entry。class的method也是callable，具有全局唯一的地址即id，和绑定到哪个实例没有关系，只有一个entry。

output::

    20:50 jaime@oldtown Python-2.6.7 (cprofile)$ ./python.exe -m cProfile test/profile.py 
    Entering func <module>
    Entering func <module>
    Entering func A
    Leaving func                    A  Timers: tt 0.0000, it 0.0000, nc 1, rl 0
    foo 4299829448
    foo1 4299808352
    A.test 4299358368
    A.test 4299358368
    Entering func test
    Entering func foo
    Entering func foo1
    Leaving func                 foo1  Timers: tt 1.0009, it 0.0000, nc 1, rl 0
    Entering func foo
    Entering func foo1
    Leaving func                 foo1  Timers: tt 2.0021, it 0.0001, nc 2, rl 0
    Entering func foo
    Entering func foo1
    Leaving func                 foo1  Timers: tt 3.0032, it 0.0001, nc 3, rl 0
    Entering func foo
    Entering func foo1
    Leaving func                 foo1  Timers: tt 4.0043, it 0.0001, nc 4, rl 0
    Leaving func                  foo  Timers: tt 0.0000, it 0.0660, nc 1, rl 3
    Leaving func                  foo  Timers: tt 0.0000, it 0.0662, nc 2, rl 2
    Leaving func                  foo  Timers: tt 0.0000, it 0.0664, nc 3, rl 1
    Leaving func                  foo  Timers: tt 4.0708, it 0.0665, nc 4, rl 0
    Leaving func                 test  Timers: tt 4.0708, it 0.0000, nc 1, rl 0
    Leaving func             <module>  Timers: tt 4.0715, it 0.0007, nc 1, rl 0
    Leaving func             <module>  Timers: tt 4.0718, it 0.0000, nc 1, rl 0
             22 function calls (19 primitive calls) in 4.072 CPU seconds

       Ordered by: standard name

       ncalls  tottime  percall  cumtime  percall filename:lineno(function)
            1    0.000    0.000    4.072    4.072 <string>:1(<module>)
            1    0.001    0.001    4.071    4.071 profile.py:1(<module>)
            1    0.000    0.000    0.000    0.000 profile.py:17(A)
            1    0.000    0.000    4.071    4.071 profile.py:18(test)
            4    0.000    0.000    4.004    1.001 profile.py:3(foo1)
          4/1    0.066    0.017    4.071    4.071 profile.py:6(foo)
            1    0.000    0.000    4.072    4.072 {execfile}
            4    0.000    0.000    0.000    0.000 {id}
            1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
            4    4.004    1.001    4.004    1.001 {time.sleep}

可以看出，每次调用foo1返回后，foo1这个entry的总耗时就加1s，foo1没有自身耗时，调用次数加1，递归深度一直为0.
而foo则不同，输出最早的那个`Leaving func foo`是最深的那次递归，递归深度rl为3，自身耗时为0.0660s，其后各次递归都没有自身耗时。当最上层foo返回即rl为0时，才计算entry foo的总耗时，为4.0708s。

对比下面的cProfile输出，可以看到tottime实际上对应于it，而不是tt，是指函数自身耗时，不包括subcall的耗时，所以可能叫inlinetime更为合适:) cumtime才是tt，函数总耗时。

Lib/cProfile.py ::

    def snapshot_stats(self):
        entries = self.getstats()
        self.stats = {}
        callersdicts = {}
        # call information
        for entry in entries:
            func = label(entry.code)
            nc = entry.callcount         # ncalls column of pstats (before '/')
            cc = nc - entry.reccallcount # ncalls column of pstats (after '/')
            tt = entry.inlinetime        # tottime column of pstats
            ct = entry.totaltime         # cumtime column of pstats

cc 为递归除外的调用次数，即4/1中的1。

statprof
~~~~~~~~~~~~~~
statprof 提供了另外一种思路。每次进行函数调用前后都执行trace操作，这算是同步的profile。如果让程序一直运行，只是定时的中断
一下，看看程序正在做什么，那么是不是可算作一种统计意义的profile？

具体做法是设置signal.SIGPROF，定时触发profile事件，在处理程序中查看当前堆栈信息，汇总之后就可大致知道程序大部分时间花在什么地方。

https://github.com/bos/statprof.py


Gevent and Gunicorn
----------------------------
gunicorn: 0.14.2, gevent: 1.0b1

gunicorn
~~~~~~~~~

gunicorn是一个WSGI server，其核心是arbiter, worker管理模型。

arbiter, 也即master进程，负责管理多个worker进程。每个worker都监听
在同一个地址上，负责处理具体的web request。这个地址可以是ip:port，
也可以是本地socket。master负责spawn，monitor, kill workers，而workers
组成一个池子， 这个进程模型非常典型。

gevent
~~~~~~

假设有greenlet F，包含三个操作A, B, C，依次顺序执行::

    greenlet F:   A -> B -> C 

如果在执行B的时候，有io数据还没就绪，则gevent会挂起当前greenlet，
转而执行别的greenlet。当发现greenlet F的io数据就绪时，会继续原来B操作。
在greenlet F看来，一切照常运行，就像阻塞了一段时间一样。这非常类似于
操作系统和进程之间的关系，当一个进程进行阻塞IO时，os挂起该进程，选择
别的进程执行，当其IO就绪后，又恢复现场继续原来的进程。
如此看来，挂起阻塞的IO，转而执行别的任务，从而使cpu不至于空等待，这也是
一个很典型的pattern。

gevent要做的事情就是patch所有的阻塞io，在其中显示调用greenlet switch，
io实际上变成异步的了，但是在greenlet内看来，结果仍是同步返回的。
如果稍有不慎，系统中仍然有遗漏的阻塞io没有patch，这个greenlet就会一直
占有cpu，导致其他greenlet无法运行，系统吞吐量则会急剧下降。

info:
串行: A, B, C 或者 A -> B -> C

并行: A | B | 或者 [A B C]

gevent(greenlet)在thread，process之外，提供了另外一种可能的并发模型。

ggevent worker
~~~~~~~~~~~~~~~~~~~
上面说到gunicorn的arbiter:worker模型，ggevent就是gunicorn支持的一种worker类型，
ggevent基于gevent，gevent基于greenlet。

http://gunicorn.org/design.html

阅读gunicorn代码请参阅 http://readthedocs.org/docs/gunicorn/en/latest/readstart.html

下面来看一下ggevent的工作流程::

    # 从Application开始
    gunicorn.app.base.WSGIApplication.run
    gunicorn.app.base.Application.run

    # 关联到一个Arbiter，启动workers
    gunicorn.arbiter.Arbiter.run
                            .manager_workers
                            .spawn_workers

    # Worker初始化
    gunicorn.workers.base.Worker.init_process
    gunicorn.workers.ggevent.GeventWorker.run:
            from gevent.pool import Pool
            from gevent.server import StreamServer

            pool = Pool(self.worker_connections)
            ...
            server = StreamServer(self.socket, handle=self.handle, spawn=pool)
            server.start()
        
Pool是gevent用来控制并发greenlet的一种机制，如果pool没有满，则pool.spawn可以立即成功，否则需要等待。 http://www.gevent.org/gevent.pool.html#gevent.pool.Pool 该参数被传递给StreamServer，用来实现并发连接数控制。

handle 参数也需注意，每个连接的具体处理，都在这个函数中完成，当server accept新连接之后，即回调此函数。

::

    gunicorn.workers.ggevent.GeventWorker.handle
    gunicorn.workers.ggevent.AsyncWorker.handle 
    gunicorn.workers.ggevent.GeventWorker.handle_request
    gunicorn.workers.ggevent.AsyncWorker.handle_request

细看handle::

    def handle(self, client, addr):
            try:
                parser = http.RequestParser(self.cfg, client)
                try:
                    while True:
                        req = None
                        with self.timeout_ctx():
                            req = parser.next()
                        if not req:
                            break
                        self.handle_request(req, client, addr)
                except StopIteration, e:
                    self.log.debug("Closing connection. %s", e)
            except socket.error, e:
                ...
            finally:
                util.close(client)

这是一个循环，从client连接中不断的读出http请求，依次处理，知道没有请求
可以读为止。这很有意思，因为它为你提供了在一个http连接中发送多个http请求
的可能性。实际上，由于client是一个普通的socket，你甚至可以不用http协议，
你可以自定义一个协议，只需将parser换成可以解析你的协议请求的parser。

pre_request, post_request钩子，具体wsgi执行都在 handle_request中。

.. note::
    
    这是一般WSGI应用的标准处理流程。和gevent worker类似的，还有一个gevent_pywsgi worker，
    它使用gevent自带的WSGI处理程序。work class为GeventPyWSGIWorker，server_class为
    gevent.pywsgi.WSGIServer，在上面创建server的时候，走的是和StreamServer不同的分支，
    在此就不深入了。

    server = self.server_class( self.socket, application=self.wsgi, spawn=pool, log=self.log, handler_class=self.wsgi_handler)
    
    application即为你的wsgi callable，handler_class则是gevent.pywsgi.WSGIHandler。        

OK, 继续看server.start的流程：

    gevent.server.StreamServer.start
    gevent.server.BaseServer.start
    gevent.server.BaseServer.start_accepting:
            if self._watcher is None:
                # just stop watcher without creating a new one?
                self._watcher = self.loop.io(self.socket.fileno(), 1)
                self._watcher.start(self._do_read)

这个watcher的作用是启动一个greenlet，利用libev来监听socket，一旦有io就调用_do_read callback，后者又调用do_handle会为每个连接启动一个新的greentlet处理::

    gevent.server.BaseServer._do_read
    gevent.server.BaseServer.do_handle

    def set_spawn(self, spawn):
        ...
        elif hasattr(spawn, 'spawn'):
            self.pool = spawn # 即上面传进来的pool参数
            self._spawn = spawn.spawn
        elif ...
        
    def do_handle(self, *args):
        spawn = self._spawn
        if spawn is None:
            self._handle(*args) # 即创建server时的handle回调函数
        else:
            spawn(self._handle, *args)

    def _do_read(self):
        for _ in xrange(self.max_accept):
            if self.full():
                self.stop_accepting()
                return
            try:
                args = self.do_read()
                self.delay = self.min_delay
                if not args:
                    return
            except:
                self.loop.handle_error(self, *sys.exc_info())
                ...
            else:
                try:
                    self.do_handle(*args)
                except:
                    self.loop.handle_error((args[1:], self), *sys.exc_info())
                    ...

_watcher.start并不是一个loop，只是spawn一个greenlet就返回了。 如果start_accepting
立即返回，start也就返回了，问：那么loop在哪里？整个server的主循环在哪里？答曰：
本来就没有loop，整个程序都是由gevent驱动greenlet的，gevent也没有loop，或者可以说,
gvent没有显式loop，整个系统是由libev的主循环驱动的::

    Unlike other network libraries and similar to eventlet, gevent starts the event 
    loop implicitly in a dedicated greenlet. There’s no reactor that you must run() or 
    dispatch() function to call. When a function from gevent API wants to block, 
    it obtains the Hub instance - a greenlet that runs the event loop - and switches to 
    it. If there’s no Hub instance yet, one is created on the fly.

http://www.gevent.org/intro.html#event-loop

更多请见下面的Hub.run。

watcher greenlet
~~~~~~~~~~~~~~~~~~

http://www.gevent.org/gevent.hub.html#module-gevent.hub

watcher.start::

    gevent.server.BaseServer:
        self.loop = gevent.get_hub().loop
        ...
        self._watcher = self.loop.io(self.socket.fileno(), 1)
        self._watcher.start(self._do_read)

    gevent.get_hub
    gevent.hub.Hub.__init__:
        loop_class = config('gevent.core.loop', 'GEVENT_LOOP')
        ...
        self.loop = loop_class(flags=loop, default=default)

gevent.core.loop在gevent/gevent/core.ppyx中定义, loop.io方法返回一个
watcher::

    gevent.core.loop.io:
        def io(self, int fd, int events, ref=True):
            return io(self, fd, events, ref)
    gevent.core.io: # 调用ev_io_init初始化fd
        libev.ev_io_init(&self._watcher, <void *>gevent_callback_io, fd, events)

watcher.start::
    gevent.core.io.start:
        self.callback = callback
        ...
        libev.ev_io_start(self.loop._ptr, &self._watcher) # 激活ev_io self._watcher

ev_io_init的回调是gevent_callback_io, 而watcher.start的回调是callback
self._do_read，这两者是怎么关联起来呢？gevent/gevent/callbacks.c::

    #define GET_OBJECT(PY_TYPE, EV_PTR, MEMBER) \
    ((struct PY_TYPE *)(((char *)EV_PTR) - offsetof(struct PY_TYPE, MEMBER)))
    ...

    #define DEFINE_CALLBACK(WATCHER_LC, WATCHER_TYPE) \
        static void gevent_callback_##WATCHER_LC(struct ev_loop *_loop, void *c_watcher, int revents) {                  \
            struct PyGevent##WATCHER_TYPE##Object* watcher = GET_OBJECT(PyGevent##WATCHER_TYPE##Object, c_watcher, _watcher);    \
            gevent_callback(watcher->loop, watcher->_callback, watcher->args, (PyObject*)watcher, c_watcher, revents); \
        }

_callback实际上就是在io.start函数中设置的callback，请参见core.ppyx中WATCHER_BASE宏定义。

ev_io_init的第一个参数，watcher._watcher，纯的裸libev.ev_io类型，当gevent_callback_io
被调用时，又被传递回来了即这个c_watcher，那么怎么找到对应的python io class对象即
watcher呢？GET_OBJECT即是答案，它可以从一个对象成员的c指针，倒推出这个对象来，强大。 

上面即是watcher.start的全部过程，get_hub自动创建了一个gevent.hub.Hub实例，一个greenlet， 整个event loop就在其Hub.run方法::

    gevent.hub.Hub.run
    gevent.core.loop.run:

        def run(self, nowait=False, once=False):
            cdef unsigned int flags = 0
            if nowait:
                flags |= libev.EVRUN_NOWAIT
            if once:
                flags |= libev.EVRUN_ONCE
            with nogil:
                libev.ev_run(self._ptr, flags)

终于，大boss出现，关于ev_run文档上这样描述::

    bool ev_run (loop, int flags)

    Finally, this is it, the event handler. This function usually is called after
    you have initialised all your watchers and you want to start handling events.
    It will ask the operating system for any new events, call the watcher
    callbacks, and then repeat the whole process indefinitely: This is why event
    loops are called loops.

http://pod.tst.eu/http://cvs.schmorp.de/libev/ev.pod

继承关系图
~~~~~~~~~~~~~~

gunicorn::

              Application
              /            \               \
      WSGIApplication  DjangoApplication   PasterBaseApplication


                   Worker
                /            \            \
            AsyncWorker     SyncWorker   TornaoWorker
               /    \            
      GeventWorker  EventletWorker


gevent::

                BaseServer
             /             \
         StreamServer     DatagramServer

         /
       WSGIServer


gunicorn reloading
~~~~~~~~~~~~~~~~~~~~~~~~
gunicorn 目前尚无自动reload机制，修改代码后需要发送SIGHUB给master进程，通知重新加载。

https://github.com/benoitc/gunicorn/issues/154

gunicorn.aribter.Arbiter init_signals 函数设置signal函数为所有信号的handler，而signal函数
只是把信号放入队列中，具体的处理统一在run函数中，这样的好处可能是降低信号handler异步执行的风险。
只有SIGCHLD信号被特殊处理。

::

    def init_signals(self):
        ...
        map(lambda s: signal.signal(s, self.signal), self.SIGNALS)
        signal.signal(signal.SIGCHLD, self.handle_chld)

    def signal(self, sig, frame):
        if len(self.SIG_QUEUE) < 5:
            self.SIG_QUEUE.append(sig)
            self.wakeup()

    def run(self):
        "Main master loop."
        self.start()
        ...
        self.manage_workers()
        while True:
            try:
                self.reap_workers()
                sig = self.SIG_QUEUE.pop(0) if len(self.SIG_QUEUE) else None
                if sig is None:
                    self.sleep()
                    self.murder_workers()
                    self.manage_workers()
                    continue
                ...
                signame = self.SIG_NAMES.get(sig)
                handler = getattr(self, "handle_%s" % signame, None)
                ...
                self.log.info("Handling signal: %s", signame)
                handler()
                self.wakeup()
                ...

    def handle_chld(self, sig, frame):
        "SIGCHLD handling"
        self.wakeup()

    def handle_hup(self):
        """\
        HUP handling.
        - Reload configuration
        - Start the new worker processes with a new configuration
        - Gracefully shutdown the old worker processes
        """
        self.log.info("Hang up: %s", self.master_name)
        self.reload()

handle_hup 负责处理HUB信号::

   def reload(self):
        ...
        # reload conf
        self.app.reload()
        self.setup(self.app)
        ...
        # spawn new workers with new app & conf
        self.cfg.on_reload(self)
        ...
        self.manage_workers()

self.app.reload在gunicorn.app.base.Application中定义，完成的工作只是重新加载app配置。

生成新的worker process是在self.cfg.on_reload，gunicorn.config::

    class OnReload(Setting):
        name = "on_reload"
        section = "Server Hooks"
        validator = validate_callable(1)
        type = "callable"
        def on_reload(server):
            for i in range(server.app.cfg.workers):
                server.spawn_worker()
        default = staticmethod(on_reload)
        desc = """\
            Called to recycle workers during a reload via SIGHUP.

            The callable needs to accept a single instance variable for the Arbiter.
            """

又生成了同样数量的worker。但是，老的worker怎么办？到此为止，好像还没有被杀掉。。。且往下看。

gunicorn.arbiter.Arbiter::

    def spawn_worker(self):
        self.worker_age += 1
        worker = self.worker_class(self.worker_age, self.pid, self.LISTENER,
                                    self.app, self.timeout/2.0,
                                    self.cfg, self.log)
        self.cfg.pre_fork(self, worker)
        pid = os.fork()
        if pid != 0:
            self.WORKERS[pid] = worker
            return pid

        # Process Child
        worker_pid = os.getpid()
        ...
 
注意worker_age这个递增id，每个master唯一，被传递给了worker_class。gunicorn.workers.base.Worker::

    class Worker(object):
        ...
        def __init__(self, age, ppid, socket, app, timeout, cfg, log):
            """\
            This is called pre-fork so it shouldn't do anything to the
            current process. If there's a need to make process wide
            changes you'll want to do that in ``self.init_process()``.
            """
            self.age = age
            ...

此时系统中有双倍的worker，下次arbiter.run循环会调用manage_worker，我们已经知道，它会保证worker数量
在可控范围之内，杀掉多余的worker, gunicorn.arbiter.Arbiter::

        def manage_workers(self):
            if len(self.WORKERS.keys()) < self.num_workers:
                self.spawn_workers()

            workers = self.WORKERS.items()
            workers.sort(key=lambda w: w[1].age)
            while len(workers) > self.num_workers:
                (pid, _) = workers.pop(0)
                self.kill_worker(pid, signal.SIGQUIT)

原来manager_workers先根据worker的age排序，然后杀掉最老的worker，这样所有发送HUB前的老worker就全被kill了，
剩下只有更新后生成的同样数量的worker，至此worker process全部完成更新。


# TODO: greenlet, libev

Worker, I will free you.

